---
title: 'Trabalho Prático 3: Comparação de Configurações do Algoritmo DE'
author: "José Joaquim de Andrade Neto, Matheus Paiva Loures, Raphael Anderson Da Silva"
date: "16 de Dezembro de 2024"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: sentence
---


```{r setup, include=FALSE}
# Função para garantir a instalação de pacotes
pacotes_necessarios <- c("dplyr", "ggplot2", "pwr", "readr", "effectsize", "effsize", "knitr","tidyr", "multcomp", "this.path", "car","ExpDE","smoof","parallel")

for (pacote in pacotes_necessarios) {
  if (!requireNamespace(pacote, quietly = TRUE)) {
    install.packages(pacote, repos = "http://cran.us.r-project.org")
  }
  library(pacote, character.only = TRUE)
}

# Configuração dos chunks do knitr
knitr::opts_chunk$set(echo = TRUE)
```


# Descrição do Problema

Este estudo de caso investiga o desempenho de duas configurações do algoritmo de Evolução Diferencial (DE) na resolução de problemas de otimização baseados na função de Rosenbrock, com dimensões variando de 2 a 150. O objetivo é avaliar a eficiência e a robustez das configurações testadas em diferentes escalas de complexidade do problema.

1.  Configurações Testadas: 
   -  Configuração 1:  `recombination_lbga` e `mutation_rand` com fator de escala `f = 4.5`.
   -  Configuração 2:  `recombination_blxAlphaBeta` (α = 0.1, β = 0.4) e `mutation_rand` com fator de escala `f = 3`.

2.  Problema de Otimização:  A função de Rosenbrock, amplamente utilizada como benchmark, gerada com o pacote `smoof`.

3.  Parâmetros do Algoritmo: 
   -  Tamanho da População:  `popsize = 5 * dim` (proporcional à dimensão do problema).
   -  Critérios de Parada:  Avaliações máximas `maxevals = 5000 * dim` e iterações máximas `maxiter = 100 * dim`.

4.  Análise de Resultados:  Os desempenhos médios das configurações foram comparados em diferentes dimensões utilizando métodos estatísticos, a fim de identificar diferenças significativas e inferir a melhor configuração para cenários variados.

Este estudo visa contribuir para a compreensão das implicações práticas das escolhas de recombinação e mutação no algoritmo DE, considerando a escalabilidade em problemas de otimização.


# Metodologia

## Teste Esploratorio

```{r}

# Definir a função de Rosenbrock
dim <- 16  # Dimensão do problema
fn <- function(X) {
  if (!is.matrix(X)) X <- matrix(X, nrow = 1)  # Garantir que X seja uma matriz
  apply(X, MARGIN = 1, FUN = smoof::makeRosenbrockFunction(dimensions = dim))
}

# Configurações
mutpars1 <- list(name = "mutation_rand", f = 4.5)
recpars1 <- list(name = "recombination_lbga")

mutpars2 <- list(name = "mutation_rand", f = 3)
recpars2 <- list(name = "recombination_blxAlphaBeta", alpha = 0.1, beta = 0.4)

# Parâmetros do problema
popsize <- 5 * dim
probpars <- list(name = "fn", xmin = rep(-5, dim), xmax = rep(10, dim))
selpars <- list(name = "selection_standard")
stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dim)

# Execução Config 1
resultados_config1 <- ExpDE(
  mutpars = mutpars1, recpars = recpars1, popsize = popsize,
  selpars = selpars, stopcrit = stopcrit, probpars = probpars
)

# Execução Config 2
resultados_config2 <- ExpDE(
  mutpars = mutpars2, recpars = recpars2, popsize = popsize,
  selpars = selpars, stopcrit = stopcrit, probpars = probpars
)

# Verificar os resultados
print(resultados_config1$Fbest)
print(resultados_config2$Fbest)
```

Os resultados obtidos indicam que a primeira configuração do algoritmo, composta pela recombinação recombination_lbga e mutação mutation_rand com fator 4.5, pode apresentar um desempenho significativamente melhor em relação à segunda configuração, que utilizou a recombinação recombination_blxAlphaBeta e mutação mutation_rand com fator 3. Esse indicativo é evidente pelos valores da função objetivo, com a primeira configuração atingindo um valor final consideravelmente inferior ao resultado obtido pela segunda, ressaltando sua maior eficácia na minimização da função de Rosenbrock no cenário analisado.


### Execução do Algoritmo para Diversas Dimensões


```{r}
dim_range <- 2:150
csv_file <- "resultados_dimensoes_corrigido.csv"

if (!file.exists(csv_file)) {
  results_list <- list()
  for (dimesion in dim_range) {
    # Definir a função de Rosenbrock
    fn <- function(X) {
      if (!is.matrix(X)) X <- matrix(X, nrow = 1)  # Garantir que X seja uma matriz
      apply(X, MARGIN = 1, FUN = smoof::makeRosenbrockFunction(dimensions = dimesion))
    }
    
    # Configurações
    mutpars1 <- list(name = "mutation_rand", f = 4.5)
    recpars1 <- list(name = "recombination_lbga")
    
    mutpars2 <- list(name = "mutation_rand", f = 3)
    recpars2 <- list(name = "recombination_blxAlphaBeta", alpha = 0.1, beta = 0.4)
    
    # Parâmetros do problema
    popsize <- 5 * dimesion
    probpars <- list(name = "fn", xmin = rep(-5, dimesion), xmax = rep(10, dimesion))
    selpars <- list(name = "selection_standard")
    stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dimesion)
    
    # Execução Config 1
    resultados_config1 <- ExpDE(
      mutpars = mutpars1, recpars = recpars1, popsize = popsize,
      selpars = selpars, stopcrit = stopcrit, probpars = probpars
    )
    
    # Execução Config 2
    resultados_config2 <- ExpDE(
      mutpars = mutpars2, recpars = recpars2, popsize = popsize,
      selpars = selpars, stopcrit = stopcrit, probpars = probpars
    )
    
    # Verificar os resultados
    print(resultados_config1$Fbest)
    print(resultados_config2$Fbest)
    
    results_list[[length(results_list) + 1]] <- data.frame(
      Dimension = dimesion,
      Best_Config1 = resultados_config1$Fbest,
      Best_Config2 = resultados_config2$Fbest
    )
    
  }
  # Combinar todos os resultados e salvar em CSV
  results_df <- do.call(rbind, results_list)
  write.csv(results_df, csv_file, row.names = FALSE)
}

results <- read.csv(csv_file)
head(results)
```


### Visualização dos Resultados


Criar um gráficos para observar tendências nas duas configurações ao longo das dimensões do problema.

```{r}

# Gráfico de linha comparando os desempenhos
ggplot(results, aes(x = Dimension)) +
  geom_line(aes(y = Best_Config1, color = "Configuração 1")) +
  geom_line(aes(y = Best_Config2, color = "Configuração 2")) +
  labs(title = "Desempenho das Configurações ao Longo das Dimensões",
       x = "Dimensão",
       y = "Melhor Resultado (Fbest)",
       color = "Configuração") +
  theme_minimal()
```
Observa-se que, de maneira geral, a Configuração 1 (recombination_lbga e mutation_rand com fator 4.5) apresenta melhores resultados em comparação com a Configuração 2 (recombination_blxAlphaBeta e mutation_rand com fator 3) em todas as dimensões avaliadas. O desempenho das duas configurações degrada progressivamente com o aumento da dimensão, o que era esperado devido à maior complexidade do problema.


```{r}
# Adicionar novas colunas ao DataFrame
adicionar_colunas <- function(data) {
  # Calcular a média das colunas Best_Config1 e Best_Config2
  data$Mean_Result <- rowMeans(data[, c("Best_Config1", "Best_Config2")])
  
  # Calcular os resíduos para cada configuração
  data$Residual_Config1 <- data$Best_Config1 - data$Mean_Result
  data$Residual_Config2 <- data$Best_Config2 - data$Mean_Result
  
  return(data)
}

# Carregar os resultados existentes
results <- read.csv(csv_file)

# Aplicar a função para adicionar novas colunas
results <- adicionar_colunas(results)
head(results)
```
```{r}
ggplot(results, aes(x = Dimension)) +
  geom_line(aes(y = Mean_Result, color = "Média")) +
  labs(
    title = "Desempenho das Configurações com Média e Resíduos ao Longo das Dimensões",
    x = "Dimensão",
    y = "Resultado (Fbest, Média ou Resíduo)",
    color = "Legenda"
  ) +
  theme_minimal()
```

A análise do gráfico apresentado demonstra que a média dos resultados dos algoritmos aumenta proporcionalmente ao número de dimensões do problema, indicando uma dependência direta entre o desempenho do algoritmo e a complexidade dimensional da função otimizada. Essa relação reflete a escalabilidade dos algoritmos avaliados, já que o aumento da dimensão torna a tarefa de encontrar soluções ótimas mais desafiadora, elevando os valores médios da função objetivo.

Para determinar qual algoritmo possui um desempenho superior, é essencial considerar a diferença entre as médias das configurações testadas. Apenas ao verificar essa diferença é possível avaliar de maneira conclusiva a eficácia relativa de cada algoritmo, pois a média reflete o desempenho médio em todos os testes realizados. Sem essa análise comparativa das médias, qualquer inferência sobre a superioridade de uma configuração seria inadequada e sujeita a vieses, ignorando o contexto da variabilidade dos resultados.


```{r}
ggplot(results, aes(x = Dimension)) +
  geom_line(aes(y = Residual_Config1, color = "Resíduo Configuração 1")) +
  geom_line(aes(y = Residual_Config2, color = "Resíduo Configuração 2")) +
  labs(
    title = "Desempenho das Configurações em relação aos Resíduos ao Longo das Dimensões",
    x = "Dimensão",
    y = "Resultado (Fbest, Média ou Resíduo)",
    color = "Legenda"
  ) +
  theme_minimal()
```


A análise da evolução dos resíduos apresentados no gráfico revela diferenças consistentes entre as configurações ao longo das dimensões. Observa-se que os resíduos da Configuração 1 apresentam valores predominantemente negativos, enquanto os da Configuração 2 são majoritariamente positivos. Isso indica que, em média, a Configuração 1 obteve resultados melhores (mais próximos ao valor mínimo da função objetivo) em comparação com a Configuração 2. 


## Cálculo do Tamanho Amostral
```{r}
 pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = "paired")

```

O cálculo do tamanho amostral indica que, para realizar um teste t pareado com uma diferença de efeito (d) de 0,5, um nível de significância de 5% (sig.level = 0,05) e um poder estatístico de 80% (power = 0,8), seriam necessárias 33,37 observações pareadas. Como o número de pares deve ser inteiro, é adequado arredondar para 34 pares. Esse resultado assegura que o teste t terá uma probabilidade de 80% de detectar um efeito de tamanho moderado (d = 0,5), caso ele exista, minimizando tanto o risco de erro tipo I (falso positivo) quanto o risco de erro tipo II (falso negativo).

## Execução Paralela do Experimento

Nesse momento é realizado os experimentos para as duas configurações do algoritmo de Evolução Diferencial (DE) para diferentes dimensões de problemas de otimização, armazenar os resultados em um arquivo CSV e paralelizar o processamento para maior eficiência. Inicialmente, ele verifica se o arquivo de resultados já existe; caso contrário, cria um novo arquivo e prepara um cabeçalho detalhado. Um cluster paralelo é configurado para processar múltiplas dimensões simultaneamente, realizando 30 execuções para cada configuração por dimensão. Os resultados são salvos incrementalmente no arquivo CSV, incluindo os valores individuais das execuções, as médias por configuração e a média geral. Dimensões que apresentarem erros durante o processamento são registradas para análise posterior. Este método eficiente garante a reprodutibilidade, escalabilidade e organização dos dados. 

```{r}
library(parallel)

# Inicializar a lista para armazenar dimensões com erros
error_dimensions <- list()

csv_file <- "resultados_dimensoes_corrigido_completo_v2.csv"

# Criar o arquivo CSV com cabeçalho, se ele não existir
if (!file.exists(csv_file)) {
  n <- 30
  dim_Total <- 2:150
  # Definir semente para garantir reprodutibilidade
  set.seed(123)
  dim_range <- sort(sample(dim_Total, size = 34, replace = FALSE), decreasing = FALSE)
  print(dim_range)
  
  # Estruturar o cabeçalho completo
  colunas_config1 <- paste0("Config1_Run_", 1:n)   # Colunas para a Configuração 1
  colunas_config2 <- paste0("Config2_Run_", 1:n)   # Colunas para a Configuração 2
  colunas_finais <- c("Mean_Config1", "Mean_Config2", "Mean_General")  # Médias
  colunas_totais <- c("Dimension", colunas_config1, colunas_config2, colunas_finais)
  
  write.csv(data.frame(matrix(ncol = length(colunas_totais), nrow = 0, dimnames = list(NULL, colunas_totais))),
            csv_file, row.names = FALSE)
  
  # Criar o cluster uma vez
  n_cores <- max(1, detectCores() - 2)  # Usar quase todos os núcleos disponíveis
  cl <- makeCluster(n_cores)
  on.exit(stopCluster(cl))  # Garantir que o cluster seja fechado no final
  
  # Lista para armazenar dimensões com erros
  error_dimensions <- list()
  
  for (dimension in dim_range) {
    print(paste("Processando dimensão:", dimension))
    
    tryCatch({
      # Definir a função de Rosenbrock
      fn <- function(X) {
        if (!is.matrix(X)) X <- matrix(X, nrow = 1)
        apply(X, MARGIN = 1, FUN = smoof::makeRosenbrockFunction(dimensions = dimension))
      }
      
      # Configurações
      mutpars1 <- list(name = "mutation_rand", f = 4.5)
      recpars1 <- list(name = "recombination_lbga")
      mutpars2 <- list(name = "mutation_rand", f = 3)
      recpars2 <- list(name = "recombination_blxAlphaBeta", alpha = 0.1, beta = 0.4)
      
      # Parâmetros do problema
      popsize <- 5 * dimension
      probpars <- list(name = "fn", xmin = rep(-5, dimension), xmax = rep(10, dimension))
      selpars <- list(name = "selection_standard")
      stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dimension)
      
      # Exportar as variáveis necessárias para o cluster
      clusterExport(cl, varlist = c("ExpDE", "mutpars1", "recpars1", "mutpars2", "recpars2", 
                                    "popsize", "selpars", "stopcrit", "probpars", "fn", "dimension"))
      
      # Função para executar as configurações paralelamente
      execute_exp <- function(i) {
        resultados_config1 <- ExpDE(mutpars = mutpars1, recpars = recpars1, popsize = popsize,
                                    selpars = selpars, stopcrit = stopcrit, probpars = probpars)
        resultados_config2 <- ExpDE(mutpars = mutpars2, recpars = recpars2, popsize = popsize,
                                    selpars = selpars, stopcrit = stopcrit, probpars = probpars)
        return(list(Config1 = resultados_config1$Fbest, Config2 = resultados_config2$Fbest))
      }
      
      # Executar paralelamente os experimentos
      results_parallel <- parLapply(cl, 1:n, execute_exp)
      
      # Separar os resultados em vetores
      results_config1 <- sapply(results_parallel, function(x) x$Config1)
      results_config2 <- sapply(results_parallel, function(x) x$Config2)
      
      # Calcular as médias
      mean_config1 <- mean(results_config1)
      mean_config2 <- mean(results_config2)
      mean_general <- mean(c(results_config1, results_config2))
      
      # Montar os resultados como uma linha do DataFrame
      result_row <- data.frame(
        Dimension = dimension,
        as.list(setNames(results_config1, colunas_config1)),
        as.list(setNames(results_config2, colunas_config2)),
        Mean_Config1 = mean_config1,
        Mean_Config2 = mean_config2,
        Mean_General = mean_general
      )
      
      # Adicionar ao arquivo CSV
      write.table(result_row, csv_file, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE)
      
    }, error = function(e) {
      message(paste("Erro na dimensão:", dimension, " - ", e))
      error_dimensions[[length(error_dimensions) + 1]] <- list(dimension = dimension, error = e)
    })
  }
  # Fechar o cluster
  stopCluster(cl)
}

# Carregar os resultados do arquivo CSV para verificar
results <- read.csv(csv_file)
print(head(results))

# Mostrar dimensões problemáticas
if (length(error_dimensions) > 0) {
  cat("Dimensões com erros:\n")
  print(error_dimensions)
} else {
  cat("Nenhuma dimensão apresentou erros.\n")
}

```


Após a coleta dos dados, o código embaralha as linhas do DataFrame para eliminar possíveis padrões que possam enviesar a análise subsequente. Em seguida, ele ajusta os valores individuais das execuções para calcular os resíduos, subtraindo as médias geral e da configuração correspondente. Essa transformação é essencial para isolar as variações residuais e realizar análises estatísticas precisas, como testes de normalidade. Este procedimento permite comparar o desempenho entre configurações de forma mais robusta e alinhada com os pressupostos estatísticos necessários para análises posteriores.

```{r}
set.seed(123) # Garantir reprodutibilidade
results_shuffled <- results[sample(nrow(results)), ] # Embaralha as linhas
head(results_shuffled)
```

```{r}
# Remover as três últimas colunas
results_individual <- results_shuffled[, -which(names(results_shuffled) %in% c("Mean_Config1", "Mean_Config2", "Mean_General"))]

# Calcular resíduos
calcular_residuos <- function(row) {
  mean_general <- row["Mean_General"]
  mean_config1 <- row["Mean_Config1"]
  mean_config2 <- row["Mean_Config2"]
  
  # Selecionar resultados individuais
  individual_results <- row[grep("Config[12]_Run_", names(row))]
  
  # Calcular resíduos para cada configuração
  residuals_config1 <- individual_results[grep("Config1_Run_", names(individual_results))] - mean_general
  residuals_config2 <- individual_results[grep("Config2_Run_", names(individual_results))] - mean_general
  
  c(residuals_config1, residuals_config2)
}

# Aplicar cálculo dos resíduos para cada linha
residuals_matrix <- t(apply(results, 1, calcular_residuos))

# Criar nomes de colunas descritivos
col_names_config1 <- names(results)[grep("Config1_Run_", names(results_shuffled))]
col_names_config2 <- names(results)[grep("Config2_Run_", names(results_shuffled))]
residual_col_names <- c(
  paste0("Residual_", col_names_config1),
  paste0("Residual_", col_names_config2)
)

# Converter para DataFrame com os novos nomes de colunas
residuals_df <- as.data.frame(residuals_matrix)
names(residuals_df) <- residual_col_names

# Verificar a estrutura do DataFrame de resíduos
head(residuals_df)

```
```{r}
# Converter residuals_df para formato longo
residuals_long <- residuals_df %>%
  mutate(Dimension = 1:nrow(residuals_df)) %>% # Adicionar coluna de dimensão
  pivot_longer(
    cols = starts_with("Residual_"), 
    names_to = "Configuration",
    values_to = "Residual"
  ) %>%
  mutate(
    Configuration_Type = ifelse(
      grepl("Config1", Configuration), 
      "Configuração 1", 
      "Configuração 2"
    )
  )

# Criar o gráfico
ggplot(residuals_long, aes(x = Dimension, y = Residual, color = Configuration_Type)) +
  geom_line(alpha = 0.7) + # Linhas para visualizar tendências
  geom_point(alpha = 0.5) + # Pontos para cada resíduo
  labs(
    title = "Distribuição de Resíduos por Configuração ao Longo das Dimensões",
    x = "Dimensão",
    y = "Resíduo",
    color = "Configuração"
  ) +
  theme_minimal() +
  theme(legend.position = "top")
```

## Teste de premissa


### Teste de Normalidade
```{r}
# Carregar todos os resíduos
residuals_matrix <- as.matrix(residuals_df)

# 1. Teste de normalidade para todos os resíduos combinados
all_residuals <- as.vector(residuals_matrix)
shapiro_all <- shapiro.test(all_residuals)
cat("Teste de Normalidade - Todos os Resíduos:\n")
print(shapiro_all)

# 2. Teste de normalidade para resíduos da Configuração 1
config1_columns <- grep("Residual_Config1", colnames(residuals_df), value = TRUE)
residuals_config1 <- as.vector(as.matrix(residuals_df[, config1_columns]))
shapiro_config1 <- shapiro.test(residuals_config1)
cat("\nTeste de Normalidade - Resíduos Configuração 1:\n")
print(shapiro_config1)

# 3. Teste de normalidade para resíduos da Configuração 2
config2_columns <- grep("Residual_Config2", colnames(residuals_df), value = TRUE)
residuals_config2 <- as.vector(as.matrix(residuals_df[, config2_columns]))
shapiro_config2 <- shapiro.test(residuals_config2)
cat("\nTeste de Normalidade - Resíduos Configuração 2:\n")
print(shapiro_config2)
```
Os resultados dos testes de normalidade indicam que, tanto para os resíduos combinados quanto para os resíduos separados por configuração, a hipótese nula do teste Shapiro-Wilk, que assume que os dados seguem uma distribuição normal, foi rejeitada (p-valor < 0.05). O valor da estatística W, próximo de 0.9 em todos os casos, reflete um desvio considerável em relação à normalidade. Esses resultados sugerem que os resíduos, tanto no conjunto total quanto em cada configuração (Configuração 1 e Configuração 2), não apresentam distribuição normal. Isso pode impactar a escolha de métodos estatísticos subsequentes, especialmente se estes assumirem normalidade nos dados, exigindo alternativas não paramétricas ou transformações nos resíduos para adequação.

### Teste de Independência

Para aplicar o teste de independência, o código mencionado calcula uma matriz de correlação, que verifica o grau de relação linear entre os resíduos de diferentes colunas. A interpretação da matriz ajudará a entender se há independência (valores de correlação próximos de 0) ou dependência (valores próximos de -1 ou 1).

```{r}
# Carregar os resíduos normalizados (se necessário)
dados_normalizados <- residuals_matrix

# Calcular a matriz de correlação para os resíduos normalizados
matriz_correlacao <- cor(dados_normalizados, method = "pearson", use = "complete.obs")

# Análise resumida dos valores de correlação
cat("\nResumo da Correlação:\n")
summary(as.vector(matriz_correlacao))
```
Os resultados da matriz de correlação indicam que os resíduos apresentam alta correlação linear entre si, com valores variando de 0,9583 a 1,0000. A mediana (0,9836) e a média (0,9830) reforçam a forte dependência linear entre as execuções analisadas. Esses resultados sugerem que os resíduos não são independentes, indicando possível interação entre os fatores analisados ou padrões sistemáticos nos dados que influenciam as diferentes execuções. Essa dependência deve ser considerada em análises subsequentes, especialmente se forem utilizados métodos que assumem independência dos resíduos, pois ela pode impactar a validade das conclusões estatísticas. Ajustes no modelo ou métodos específicos para tratar a dependência podem ser necessários.

### Teste de Homogeneidade de Variância:

```{r}

dados_longo <- pivot_longer(
  residuals_df, 
  cols = starts_with("Residual_Config"), 
  names_to = c("Configuração", "Execução"),
  names_pattern = "Residual_(Config\\d+)_Run_(\\d+)",
  values_to = "Resíduo"
)


leveneTest(Resíduo ~ Configuração, data = dados_longo)
```
O Teste de Levene foi aplicado para verificar a homogeneidade de variâncias entre os grupos analisados, com o uso da mediana como medida de centralidade. O teste resultou em um valor de p-valor significativamente menor do que o nível de significância adotado de  α = 0,05. Dessa forma, rejeita-se a hipótese nula do teste (H₀), que afirma que as variâncias dos grupos são homogêneas. Isso indica que há diferenças significativas nas variâncias entre os grupos analisados.

### Conclusão do Teste de premissa


Os testes estatísticos realizados indicaram que os resíduos não seguem uma distribuição normal (Shapiro-Wilk com p-valor < 0,05), apresentam alta dependência linear (média e mediana das correlações próximas a 0,98), e não possuem homogeneidade de variâncias entre as configurações (Teste de Levene com p-valor < 0,001). Esses resultados invalidam o uso do ANOVA tradicional, que assume normalidade, independência e homogeneidade de variâncias. Como alternativa, recomenda-se o uso de métodos não paramétricos, como o Teste de Wilcoxon para comparações pareadas entre as configurações ou o Teste de Kruskal-Wallis para múltiplos grupos.


## Teste não paramétricos 
Para determinar qual das duas configurações (Config1 e Config2) apresenta melhor desempenho em todos os cenários, optamos pelo Teste de Permutação. Esse método foi escolhido devido às características dos dados, que não atendem às premissas de normalidade e homogeneidade de variâncias, conforme verificado nos testes preliminares (Shapiro-Wilk e Levene). O Teste de Permutação é robusto, não exige suposições rígidas sobre a distribuição dos dados e permite a comparação direta das médias entre as configurações. Ele utiliza reamostragens para construir uma distribuição empírica da estatística de teste, fornecendo uma análise confiável para identificar se as diferenças observadas entre as configurações são estatisticamente significativas.

### Formulação do Teste de Hipótese

A comparação entre as configurações será feita utilizando a  diferença média  entre os resíduos como estatística de interesse. As hipóteses são formuladas da seguinte maneira:

-  Hipótese Nula (H0) : Não há diferença significativa entre as médias das duas configurações.
  \[
  H_0: \mu_{\text{Config1}} = \mu_{\text{Config2}}
  \]
-  Hipótese Alternativa (H1) : Há uma diferença significativa entre as médias das duas configurações.
  \[
  H_1: \mu_{\text{Config1}} \neq \mu_{\text{Config2}}
  \]

A análise será realizada utilizando  nível de significância  \(\alpha = 0.05\), garantindo que a probabilidade de um erro tipo I (rejeitar H0 quando verdadeira) não ultrapasse 5%.

### Teste de Hipótese

```{r}
# Inicializar lista para armazenar resultados de cada dimensão
dimensoes <- unique(results_shuffled$Dimension) # Obter valores únicos de dimensão
n_perm <- 100000  # Número de permutações
resultados_dimensao <- data.frame(Dimensao = integer(), p_value = numeric())

# Loop para realizar o Teste de Permutação em cada dimensão
for (dim in dimensoes) {
  dim <- as.numeric(dim)  # Garantir que dim é numérico
  
  # Filtrar linhas correspondentes à dimensão atual
  linhas_dim <- results_shuffled[results_shuffled$Dimension == dim, ]
  
  # Verificar se a coluna "Mean_General" existe e extrair sua média
  mean_general <- linhas_dim$Mean_General[1] 
  
  # Extrair resíduos para Config1 e Config2 e subtrair a média global
  config1_cols <- grep("Config1_Run_", colnames(linhas_dim), value = TRUE)
  config2_cols <- grep("Config2_Run_", colnames(linhas_dim), value = TRUE)
  
  config1 <- as.vector(as.matrix(linhas_dim[, config1_cols])) - mean_general
  config2 <- as.vector(as.matrix(linhas_dim[, config2_cols])) - mean_general  
  
  t_hat <- mean(config1) - mean(config2)
  combined_data <- append(config1, config2)
  n1 <- length(config1)
  n2 <- length(config2)
  t_perm <- numeric(n_perm)
  
  # Realizar Teste de Permutação
  set.seed(123 + dim)  # Reprodutibilidade
  for (i in 1:n_perm) {
      S_perm <- sample(combined_data)  # Embaralhar os dados
      X_perm <- S_perm[1:n1]  # Novos valores para config1
      Y_perm <- S_perm[(n1 + 1):(n1 + n2)]  # Novos valores para config2
      t_perm[i] <- mean(X_perm) - mean(Y_perm)
  }
    
  # Calcular p-valor (H1: mu_config1 > mu_config2)
  p_value <- (1 + sum(abs(t_perm) >= abs(t_hat))) / (1 + n_perm)  
  resultados_dimensao <- rbind(resultados_dimensao, data.frame(Dimensao = dim, p_value = p_value))
 
}
resultados_dimensao$p_value <- formatC(as.numeric(resultados_dimensao$p_value), format = "e", digits = 10)

# Imprimir resultados
print(resultados_dimensao)
```

Os resultados obtidos a partir do  Teste de Permutação  evidenciam diferenças estatisticamente significativas entre as configurações  Config1  e  Config2  em todas as dimensões analisadas. Com p-valores extremamente baixos, próximos de \(9.9999 \times 10^{-6}\), para todas as dimensões testadas  é possível rejeitar a  Hipótese Nula (\(H_0\))  com alto grau de confiança (\(\alpha = 0.05\)). Esses valores indicam que as diferenças observadas entre as médias das duas configurações não são atribuíveis ao acaso e, portanto, apontam um  desempenho significativamente distinto  entre Config1 e Config2. Isso reforça a robustez do método aplicado e a confiabilidade dos resultados obtidos.




```{r}
# Combinar resíduos de todas as dimensões para Config1 e Config2
config1_global <- as.vector(as.matrix(residuals_df[, grep("Residual_Config1", colnames(residuals_df))]))
config2_global <- as.vector(as.matrix(residuals_df[, grep("Residual_Config2", colnames(residuals_df))]))

# Calcular a diferença média observada global
obs_diff_global <- mean(config1_global) - mean(config2_global)

# Combinar todos os dados em um único vetor para permutação
combined_data_global <- c(config1_global, config2_global)
n_config1_global <- length(config1_global)
n_perm <- 100000  # Número de permutações
perm_diffs_global <- numeric(n_perm)  # Inicializar vetor para armazenar permutações

# Realizar Teste de Permutação Global
set.seed(999)  # Garantir reprodutibilidade
for (i in 1:n_perm) {
  shuffled_data <- sample(combined_data_global)  # Embaralhar os dados
  perm_config1 <- shuffled_data[1:n_config1_global]  # Nova amostra Config1
  perm_config2 <- shuffled_data[(n_config1_global + 1):length(shuffled_data)]  # Nova amostra Config2
  perm_diffs_global[i] <- mean(perm_config1) - mean(perm_config2)  # Estatística de permutação
}


p_value_global <- (1 + sum(abs(perm_diffs_global) >= abs(obs_diff_global))) / (1 + n_perm) 
# Exibir resultados globais
cat("### Teste de Permutação Global ###\n")
cat("Diferença média observada (global):", obs_diff_global, "\n")
cat("P-valor (global):", p_value_global, "\n")

```

Com base nos resultados do  Teste de Permutação Global , a  diferença média observada  de  -1.706.261  indica que os tempos da  Config2  foram significativamente maiores do que os da  Config1 . Como o desempenho é avaliado pelo tempo e  quanto maior o tempo, pior a solução , conclui-se que a  Config2  apresentou um desempenho  muito inferior  em relação à  Config1 . O p-valor extremamente baixo ( 9.9999 × 10⁻⁶ ) reforça que essa diferença não é fruto do acaso, sendo estatisticamente significativa. Assim, a  Config1  se destaca como a melhor solução, apresentando tempos menores e, consequentemente, maior eficiência em comparação à  Config2 .


```{r}
# Dados de exemplo: ajuste para usar seus próprios dados
config1_times <- as.vector(as.matrix(residuals_df[, grep("Residual_Config1", colnames(residuals_df))]))
config2_times <- as.vector(as.matrix(residuals_df[, grep("Residual_Config2", colnames(residuals_df))]))

# Criar um dataframe consolidado
data_visual <- data.frame(
  Tempo = c(config1_times, config2_times),
  Configuração = rep(c("Config1", "Config2"), c(length(config1_times), length(config2_times)))
)

# Gráfico de Boxplot
ggplot(data_visual, aes(x = Configuração, y = Tempo, fill = Configuração)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Comparação de Tempos entre Config1 e Config2",
       x = "Configuração",
       y = "Tempo",
       fill = "Configuração") +
  scale_fill_manual(values = c("Config1" = "skyblue", "Config2" = "tomato")) +
  theme(text = element_text(size = 12))

```
O gráfico de caixas apresenta uma comparação visual dos tempos entre as configurações Config1 e Config2. Observa-se que a Config1 possui uma mediana significativamente menor, além de uma distribuição mais concentrada abaixo do eixo zero, indicando tempos menores e mais consistentes. Em contraste, a Config2 apresenta valores de tempo consideravelmente mais elevados e maior dispersão, evidenciando um desempenho inferior. A presença de outliers na Config1 sugere alguns valores atípicos, porém eles não comprometem a tendência geral. Assim, a análise confirma que a Config1 supera a Config2, apresentando tempos menores e, consequentemente, melhor desempenho.

### Teste de Wilcoxon

```{r}
# Teste de Wilcoxon
wilcox_result <- wilcox.test(config1_global, config2_global, paired = TRUE, alternative = "less")

# Exibir resultado
print(wilcox_result)

```

O resultado do  Teste de Wilcoxon com correção de continuidade  reforça as conclusões obtidas anteriormente. Com um  p-valor extremamente baixo  (p < 2.2 × 10⁻¹⁶), rejeita-se a  Hipótese Nula (\(H_0\))  de que não há diferença significativa entre as configurações. A hipótese alternativa, que indica que os tempos da  Config1  são  significativamente menores  do que os da  Config2 , é corroborada. O valor da estatística \(V = 264\) e o resultado do teste confirmam que a  Config1  apresenta desempenho  superior , com tempos mais baixos, enquanto a  Config2  demonstra desempenho inferior, validando as análises anteriores de maneira robusta.
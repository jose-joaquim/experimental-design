---
title: 'Trabalho Prático 3: Comparação de Configurações do Algoritmo DE'
author: "José Joaquim de Andrade Neto, Matheus Paiva Loures, Raphael Anderson Da Silva"
date: "16 de Dezembro de 2024"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: sentence
bibliography: tp3.bib
---

```{r setup, include=FALSE}
# Função para garantir a instalação de pacotes
pacotes_necessarios <- c("dplyr", "ggplot2", "pwr", "readr", "effectsize", "effsize", "knitr","tidyr", "multcomp", "this.path", "car","ExpDE","smoof","parallel")

for (pacote in pacotes_necessarios) {
  if (!requireNamespace(pacote, quietly = TRUE)) {
    install.packages(pacote, repos = "http://cran.us.r-project.org")
  }
  library(pacote, character.only = TRUE)
}

# Configuração dos chunks do knitr
knitr::opts_chunk$set(echo = TRUE)
```

# Descrição do Problema

Este estudo de caso investiga o desempenho de duas configurações distintas do algoritmo de Evolução Diferencial (DE - do inglês *differential evolution*) na resolução de problemas de otimização baseados na função de Rosenbrock.
Nesse estudo de caso, queremos avaliar qual é a melhor configuração na busca pela solução que minimiza o valor da função objetivo.
Dessa forma, quanto menor o valor alcançado, melhor o resultado.
Ao fim desse estudo de caso, esperamos contribuir para com a compreensão das implicações práticas das escolhas de recombinação e mutação no algoritmo DE, considerando a **escalabilidade** em problemas de otimização.

## A função de Rosenbrock

A função de Rosenbrock é uma função não-convexa e monomodal que é comumente utilizada para testar o desempenho de algortimos de otimização.
Ela se caracteriza por possuir um mínimo global em um vale plano, longo e estreito que o torna de difícil convergência.
Inicialmente definida para duas dimensões, possui variantes que a permite ser avaliada em dimensões maiores, tornando o problema de otimização muito mais complexo.

## O algoritmo *Differential Evolution*

De acordo com a taxonomia utilizada no livro-texto de @Coello2007, o DE é uma meta-heurística evolucionária, originalmente desenvolvida para a resolução de problemas de otimização em espaço de buscas contínuo.
Algoritmos classificados como _evolucionários_ otimizam um dado problema combinatório através da manutenção das chamadas populações.
Uma população é um conjunto de invíduos - soluções para o problema que está sendo otimizado - que podem ser gerados aleatoriamente e em seguida são constantemente modificados com a utilização de operadores especificados nos parâmetros de entrada do algoritmo.
Nesse estudo de caso, focamos nos operadores de mutação (aqueles que modificam - ou perturbam - um indivíduo) e de recombinação (aqueles que selecionam um subconjunto de invidíduos para gerar um descendente).

As duas configurações do DE testadas neste estudo de caso diferenciam-se entre si nos operadores escolhidos para as operações de mutação e de recombinação.
Na **Configuração 1**, é utilizado um operador de mutação do tipo aleatório com fator de escala $f=4.5$, enquanto que a recombinação é do tipo LBGA (_Linear Breeder Genetic Algorithm_).
Já para a **Configuração 2**, temos a mutação com $f=3$ e recombinação do tipo BLX (_Blend Alpha Beta_).


# Planejamento do Experimento

Visando uma maior fundamentação das conclusões extraídas desse estudo de caso acerca da melhor configuração do DE, optamos pela realização de um experimento estatístico baseado em _blocagem_, detalhada no livro-texto [@Montgomery2010] da disciplina. Além disso, nosso experimento também é baseado nos _guidelines_ apresentados no _paper_ de @Campelo2019. A seguir, apresentamos a metodologia utilizada no experimento realizado nesse estudo de caso.

## Teste de Hipótese

O tipo de experimento estudado sugere que seja usado o teste pareado para comparar a diferença de performance entre as duas configurações do DE em diversas instâncias.
Sendo assim, considerando o conjunto de instâncias $\Gamma = \{\gamma_1, \gamma_2,...\}$, onde cada elemento $\gamma_j$ representa a função de Rosenbrock em uma dada dimensão, e o conjunto de algoritmos formado pelas duas configurações $\Lambda = \{a_{c1}, a_{c2}\}$, estamos interessados na média $\mu_D$ da distribuição $P(\Phi = \{\phi_1, \phi_2, \cdots \, \phi_N\})$, tal que

$$
\phi_j = x_{1j}-x_{2j},
$$

onde $x_{1j}$ e $x_{2j}$ são as medidas de performance do algoritmo 1 e 2 na instância $j$ e $\phi_j$ é a diferença entre elas.

Como a classe de algoritmo a qual o DE pertence tem como característica possuirem um componente aleatório em seu resultado, $x_{1j}$, $x_{2j}$ e $\phi_j$ comportam-se como variáveis aleatórias.
Dessa forma, a diferença de performance é melhor representada como sendo

$$
\phi_j = \overline{X}_{1j}-\overline{X}_{2j}
$$

sendo $\overline{X}_{ij}$ a média amostral da performance do algoritmo $i$ na instância $j$, obtida após $n_{ij}$ repetições.

Podemos então definir o seguinte teste de hipóteses em relação ao $\mu_D$

$$
H_0: \mu_D = 0 \\
H_1: \mu_D \neq 0
$$

Com base nisso, com um nível de significância $\alpha = 0,05$ e poder do teste $\pi = 1-\beta=0,8$, estamos interessados em em detectar uma diferença de performance entre os algoritmos tal que o tamanho de efeito patronizado mínimo seja $d^* = δ∗/σ = 0.5$.

## Tamanho Amostral

O cálculo do tamanho amostral a seguir mostra que, para realizar um teste-t pareado com uma diferença de efeito (d) de $0.5$, um nível de significância de $5\%$ (sig.level = $0.05$) e um poder estatístico de $80\%$ (_power_ $= 0.8$), são necessárias $33.37$ observações pareadas.
Como o número de pares deve ser inteiro, este foi arredondado para $34$.
Isso indica que é necessário avaliar as duas configurações do algoritmo em 34 instâncias diferentes (dimensões da função Rosenbrock), ou seja, que teremos $|\Gamma| = N = 34$.

```{r}
pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = "paired")
```

As 34 instâncias - ou dimensões da função Rosenbrock - foram escolhidas de forma aleatória dentro do intervalo [2,150], conforme abaixo:

```{r}
intervalo <- 2:150
N = 34
set.seed(123) # para garantir reproducibilidade
instancias <- sort(sample(intervalo, size = N, replace = FALSE), decreasing = FALSE)
```

A incerteza associada a $\overline{X}_{ij}$ devido ao número finito de observações $n_{ij}$ afeta diretamente a estimação de $\phi_j$, que por sua vez influencia no poder do teste estatístico sobre $\mu_D$.
Deseja-se então obter o número de repetições $n_{ij}$ tal que a sua variância seja irrelevante perante a variância do conjunto $\Phi$.

É importante ressaltarmos que, uma vez que não temos conhecimento prévio sobre a distribuição de probabilidade de $x_{ij}$, decidimos por fazer $n_{ij} = 30$, ou seja, rodar cada algoritmo 30 vezes em cada uma das 34 instâncias.


# Coleta dos Dados

Apesar de termos definido a quantidade de instâncias ($N=34$) e o número de repetições ($n_{ij} = 30$), decidimos fazer uma varredura exploratória da performance dos dois algoritmos em todas as dimensões do intervalo $[2,150]$, fazendo apenas uma repetição ($n_{ij} = 1$).
Isso nos deu uma primeira impressão sobre como é o comportamento da performance com o aumento da dimensão.

Após essa primeira análise, o experimento completo foi executado e os dados, necessários para a análise estatística, coletados e guardados em arquivos CSV.
Para reduzir o tempo total de execução do experimento, que alcança várias horas, principalmente por causa das dimensões maiores, optamos por usar uma extratégia de processamento paralelo.

## Análise Exploratória

### Avaliação Inicial de Otimização da Função de Rosenbrok

```{r}

# Definir a função de Rosenbrock
dim <- 16  # Dimensão do problema
fn <- function(X) {
  if (!is.matrix(X)) X <- matrix(X, nrow = 1)  # Garantir que X seja uma matriz
  apply(X, MARGIN = 1, FUN = smoof::makeRosenbrockFunction(dimensions = dim))
}

# Configurações
mutpars1 <- list(name = "mutation_rand", f = 4.5)
recpars1 <- list(name = "recombination_lbga")

mutpars2 <- list(name = "mutation_rand", f = 3)
recpars2 <- list(name = "recombination_blxAlphaBeta", alpha = 0.1, beta = 0.4)

# Parâmetros do problema
popsize <- 5 * dim
probpars <- list(name = "fn", xmin = rep(-5, dim), xmax = rep(10, dim))
selpars <- list(name = "selection_standard")
stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dim)

# Execução Config 1
resultados_config1 <- ExpDE(
  mutpars = mutpars1, recpars = recpars1, popsize = popsize,
  selpars = selpars, stopcrit = stopcrit, probpars = probpars
)

# Execução Config 2
resultados_config2 <- ExpDE(
  mutpars = mutpars2, recpars = recpars2, popsize = popsize,
  selpars = selpars, stopcrit = stopcrit, probpars = probpars
)

# Verificar os resultados
print(resultados_config1$Fbest)
print(resultados_config2$Fbest)
```

Os resultados obtidos indicam que para uma única execução em uma dimensão qualquer (16) a primeira configuração do algoritmo, pode apresentar um desempenho significativamente melhor em relação à segunda configuração.
Isso é evidente pelos valores da função objetivo, com a primeira configuração atingindo um valor final consideravelmente inferior ao resultado obtido pela segunda ($3161$ contra $207750$), o que é um indicativo inicial de sua melhor performance na minimização da função de Rosenbrock no cenário analisado.

### Avalição da Relação Entre Dimensão e Performance

```{r}
dim_range <- 2:150
csv_file <- "resultados_dimensoes_corrigido.csv"

if (!file.exists(csv_file)) {
  results_list <- list()
  for (dimesion in dim_range) {
    # Definir a função de Rosenbrock
    fn <- function(X) {
      if (!is.matrix(X)) X <- matrix(X, nrow = 1)  # Garantir que X seja uma matriz
      apply(X, MARGIN = 1, FUN = smoof::makeRosenbrockFunction(dimensions = dimesion))
    }
    
    # Configurações
    mutpars1 <- list(name = "mutation_rand", f = 4.5)
    recpars1 <- list(name = "recombination_lbga")
    
    mutpars2 <- list(name = "mutation_rand", f = 3)
    recpars2 <- list(name = "recombination_blxAlphaBeta", alpha = 0.1, beta = 0.4)
    
    # Parâmetros do problema
    popsize <- 5 * dimesion
    probpars <- list(name = "fn", xmin = rep(-5, dimesion), xmax = rep(10, dimesion))
    selpars <- list(name = "selection_standard")
    stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dimesion)
    
    # Execução Config 1
    resultados_config1 <- ExpDE(
      mutpars = mutpars1, recpars = recpars1, popsize = popsize,
      selpars = selpars, stopcrit = stopcrit, probpars = probpars
    )
    
    # Execução Config 2
    resultados_config2 <- ExpDE(
      mutpars = mutpars2, recpars = recpars2, popsize = popsize,
      selpars = selpars, stopcrit = stopcrit, probpars = probpars
    )
    
    # Verificar os resultados
    print(resultados_config1$Fbest)
    print(resultados_config2$Fbest)
    
    results_list[[length(results_list) + 1]] <- data.frame(
      Dimension = dimesion,
      Best_Config1 = resultados_config1$Fbest,
      Best_Config2 = resultados_config2$Fbest
    )
    
  }
  # Combinar todos os resultados e salvar em CSV
  results_df <- do.call(rbind, results_list)
  write.csv(results_df, csv_file, row.names = FALSE)
}

results <- read.csv(csv_file)
head(results)
```

```{r}

# Gráfico de linha comparando os desempenhos
ggplot(results, aes(x = Dimension)) +
  geom_line(aes(y = Best_Config1, color = "Configuração 1")) +
  geom_line(aes(y = Best_Config2, color = "Configuração 2")) +
  labs(title = "Desempenho das Configurações ao Longo das Dimensões",
       x = "Dimensão",
       y = "Mínimo Obtido da Função de Rosenbrock",
       color = "Configuração") +
  theme_minimal()
```

Observa-se que, de maneira geral, a Configuração 1 apresentou melhores resultados em comparação com a Configuração 2 em todas as dimensões avaliadas.
O desempenho das duas configurações degrada progressivamente com o aumento da dimensão, o que era esperado devido à maior complexidade do problema.
Destaca-se também o comportamento não linear das curvas, visualmente próxima a uma exponencial.

```{r}
# Adicionar novas colunas ao DataFrame
adicionar_colunas <- function(data) {
  # Calcular a média das colunas Best_Config1 e Best_Config2
  data$Mean_Result <- rowMeans(data[, c("Best_Config1", "Best_Config2")])
  
  # Calcular os resíduos para cada configuração
  data$Residual_Config1 <- data$Best_Config1 - data$Mean_Result
  data$Residual_Config2 <- data$Best_Config2 - data$Mean_Result
  
  return(data)
}

# Carregar os resultados existentes
results <- read.csv(csv_file)

# Aplicar a função para adicionar novas colunas
results <- adicionar_colunas(results)
head(results)
```

```{r}
ggplot(results, aes(x = Dimension)) +
  geom_line(aes(y = Mean_Result, color = "Média")) +
  labs(
    title = "Desempenho das Configurações com Média e Resíduos ao Longo das Dimensões",
    x = "Dimensão",
    y = "Resultado (Fbest, Média ou Resíduo)",
    color = "Legenda"
  ) +
  theme_minimal()
```

A análise do gráfico apresentado demonstra que a média dos resultados dos algoritmos aumenta proporcionalmente ao número de dimensões do problema, indicando uma dependência direta entre o desempenho do algoritmo e a complexidade dimensional da função otimizada.
Essa relação reflete a escalabilidade dos algoritmos avaliados, já que o aumento da dimensão torna a tarefa de encontrar soluções ótimas mais desafiadora, elevando os valores médios da função objetivo.

Para determinar qual algoritmo possui um desempenho superior, é essencial considerar a diferença entre as médias das configurações testadas.
Apenas ao verificar essa diferença é possível avaliar de maneira conclusiva a eficácia relativa de cada algoritmo, pois a média reflete o desempenho médio em todos os testes realizados.
Sem essa análise comparativa das médias, qualquer inferência sobre a superioridade de uma configuração seria inadequada e sujeita a vieses, ignorando o contexto da variabilidade dos resultados.

```{r}
ggplot(results, aes(x = Dimension)) +
  geom_line(aes(y = Residual_Config1, color = "Resíduo Configuração 1")) +
  geom_line(aes(y = Residual_Config2, color = "Resíduo Configuração 2")) +
  labs(
    title = "Desempenho das Configurações em relação aos Resíduos ao Longo das Dimensões",
    x = "Dimensão",
    y = "Resultado (Fbest, Média ou Resíduo)",
    color = "Legenda"
  ) +
  theme_minimal()
```

A análise da evolução dos resíduos apresentados no gráfico revela diferenças consistentes entre as configurações ao longo das dimensões.
Observa-se que os resíduos da Configuração 1 apresentam valores predominantemente negativos, enquanto os da Configuração 2 são majoritariamente positivos.
Isso indica que, em média, a Configuração 1 obteve resultados melhores (mais próximos ao valor mínimo da função objetivo) em comparação com a Configuração 2.


## Execução do Experimento

Nesse momento realizamos os experimentos para as duas configurações do algoritmo DE nas 34 dimensões sorteadas aleatoriamentede.
Os resultados foram armazenados em um arquivo CSV.
O processamento foi paralelizado para aumentar eficiência da execução do código.
Inicialmente, o código verifica se o arquivo de resultados já existe; caso contrário, cria um novo arquivo e prepara um cabeçalho detalhado.
Um cluster paralelo é configurado para processar múltiplas dimensões simultaneamente, realizando as execuções para cada configuração por dimensão.
Os resultados são salvos incrementalmente no arquivo CSV, incluindo os valores individuais das execuções, as médias por configuração e a média geral.
Dimensões que apresentarem erros durante o processamento são registradas para análise posterior.
Este método eficiente garante a reprodutibilidade, escalabilidade e organização dos dados.

```{r}
library(parallel)

# Inicializar a lista para armazenar dimensões com erros
error_dimensions <- list()

csv_file <- "resultados_dimensoes_corrigido_completo_v2.csv"

# Criar o arquivo CSV com cabeçalho, se ele não existir
if (!file.exists(csv_file)) {
  n <- 30
  dim_Total <- 2:150
  # Definir semente para garantir reprodutibilidade
  set.seed(123)
  dim_range <- sort(sample(dim_Total, size = 34, replace = FALSE), decreasing = FALSE)
  print(dim_range)
  
  # Estruturar o cabeçalho completo
  colunas_config1 <- paste0("Config1_Run_", 1:n)   # Colunas para a Configuração 1
  colunas_config2 <- paste0("Config2_Run_", 1:n)   # Colunas para a Configuração 2
  colunas_finais <- c("Mean_Config1", "Mean_Config2", "Mean_General")  # Médias
  colunas_totais <- c("Dimension", colunas_config1, colunas_config2, colunas_finais)
  
  write.csv(data.frame(matrix(ncol = length(colunas_totais), nrow = 0, dimnames = list(NULL, colunas_totais))),
            csv_file, row.names = FALSE)
  
  # Criar o cluster uma vez
  n_cores <- max(1, detectCores() - 2)  # Usar quase todos os núcleos disponíveis
  cl <- makeCluster(n_cores)
  on.exit(stopCluster(cl))  # Garantir que o cluster seja fechado no final
  
  # Lista para armazenar dimensões com erros
  error_dimensions <- list()
  
  for (dimension in dim_range) {
    print(paste("Processando dimensão:", dimension))
    
    tryCatch({
      # Definir a função de Rosenbrock
      fn <- function(X) {
        if (!is.matrix(X)) X <- matrix(X, nrow = 1)
        apply(X, MARGIN = 1, FUN = smoof::makeRosenbrockFunction(dimensions = dimension))
      }
      
      # Configurações
      mutpars1 <- list(name = "mutation_rand", f = 4.5)
      recpars1 <- list(name = "recombination_lbga")
      mutpars2 <- list(name = "mutation_rand", f = 3)
      recpars2 <- list(name = "recombination_blxAlphaBeta", alpha = 0.1, beta = 0.4)
      
      # Parâmetros do problema
      popsize <- 5 * dimension
      probpars <- list(name = "fn", xmin = rep(-5, dimension), xmax = rep(10, dimension))
      selpars <- list(name = "selection_standard")
      stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dimension)
      
      # Exportar as variáveis necessárias para o cluster
      clusterExport(cl, varlist = c("ExpDE", "mutpars1", "recpars1", "mutpars2", "recpars2", 
                                    "popsize", "selpars", "stopcrit", "probpars", "fn", "dimension"))
      
      # Função para executar as configurações paralelamente
      execute_exp <- function(i) {
        resultados_config1 <- ExpDE(mutpars = mutpars1, recpars = recpars1, popsize = popsize,
                                    selpars = selpars, stopcrit = stopcrit, probpars = probpars)
        resultados_config2 <- ExpDE(mutpars = mutpars2, recpars = recpars2, popsize = popsize,
                                    selpars = selpars, stopcrit = stopcrit, probpars = probpars)
        return(list(Config1 = resultados_config1$Fbest, Config2 = resultados_config2$Fbest))
      }
      
      # Executar paralelamente os experimentos
      results_parallel <- parLapply(cl, 1:n, execute_exp)
      
      # Separar os resultados em vetores
      results_config1 <- sapply(results_parallel, function(x) x$Config1)
      results_config2 <- sapply(results_parallel, function(x) x$Config2)
      
      # Calcular as médias
      mean_config1 <- mean(results_config1)
      mean_config2 <- mean(results_config2)
      mean_general <- mean(c(results_config1, results_config2))
      
      # Montar os resultados como uma linha do DataFrame
      result_row <- data.frame(
        Dimension = dimension,
        as.list(setNames(results_config1, colunas_config1)),
        as.list(setNames(results_config2, colunas_config2)),
        Mean_Config1 = mean_config1,
        Mean_Config2 = mean_config2,
        Mean_General = mean_general
      )
      
      # Adicionar ao arquivo CSV
      write.table(result_row, csv_file, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE)
      
    }, error = function(e) {
      message(paste("Erro na dimensão:", dimension, " - ", e))
      error_dimensions[[length(error_dimensions) + 1]] <- list(dimension = dimension, error = e)
    })
  }
  # Fechar o cluster
  stopCluster(cl)
}

# Carregar os resultados do arquivo CSV para verificar
results <- read.csv(csv_file)
print(head(results))

# Mostrar dimensões problemáticas
if (length(error_dimensions) > 0) {
  cat("Dimensões com erros:\n")
  print(error_dimensions)
} else {
  cat("Nenhuma dimensão apresentou erros.\n")
}

```

Após a coleta dos dados, o código embaralha as linhas do DataFrame para eliminar possíveis padrões que possam enviesar a análise subsequente.
Em seguida, ele ajusta os valores individuais das execuções para calcular os resíduos, subtraindo as médias geral e da configuração correspondente.
Essa transformação é essencial para isolar as variações residuais e realizar análises estatísticas precisas, como testes de normalidade.
Este procedimento permite comparar o desempenho entre configurações de forma mais robusta e alinhada com os pressupostos estatísticos necessários para análises posteriores.

```{r}
set.seed(123) # Garantir reprodutibilidade
results_shuffled <- results[sample(nrow(results)), ] # Embaralha as linhas
head(results_shuffled)
```

```{r}
# Remover as três últimas colunas
results_individual <- results_shuffled[ -which(names(results_shuffled) %in% c("Mean_Config1", "Mean_Config2", "Mean_General"))]

# Calcular resíduos
calcular_residuos <- function(row) {
  mean_general <- row["Mean_General"]
  mean_config1 <- row["Mean_Config1"]
  mean_config2 <- row["Mean_Config2"]
  
  # Selecionar resultados individuais
  individual_results <- row[grep("Config[12]_Run_", names(row))]
  
  # Calcular resíduos para cada configuração
  residuals_config1 <- individual_results[grep("Config1_Run_", names(individual_results))] - mean_general
  residuals_config2 <- individual_results[grep("Config2_Run_", names(individual_results))] - mean_general
  
  c(residuals_config1, residuals_config2)
}

# Aplicar cálculo dos resíduos para cada linha
residuals_matrix <- t(apply(results, 1, calcular_residuos))

# Criar nomes de colunas descritivos
col_names_config1 <- names(results)[grep("Config1_Run_", names(results_shuffled))]
col_names_config2 <- names(results)[grep("Config2_Run_", names(results_shuffled))]
residual_col_names <- c(
  paste0("Residual_", col_names_config1),
  paste0("Residual_", col_names_config2)
)

# Converter para DataFrame com os novos nomes de colunas
residuals_df <- as.data.frame(residuals_matrix)
names(residuals_df) <- residual_col_names

# Adicionar a coluna "Dimension" no DataFrame final
residuals_df <- cbind(Dimension = results_shuffled$Dimension, residuals_df)

# Verificar a estrutura do DataFrame de resíduos
head(residuals_df)

```

```{r}
# Converter residuals_df para formato longo
residuals_long <- residuals_df %>%
  mutate(Dimension = 1:nrow(residuals_df)) %>% # Adicionar coluna de dimensão
  pivot_longer(
    cols = starts_with("Residual_"), 
    names_to = "Configuration",
    values_to = "Residual"
  ) %>%
  mutate(
    Configuration_Type = ifelse(
      grepl("Config1", Configuration), 
      "Configuração 1", 
      "Configuração 2"
    )
  )

# Criar o gráfico
ggplot(residuals_long, aes(x = Dimension, y = Residual, color = Configuration_Type)) +
  geom_line(alpha = 0.7) + # Linhas para visualizar tendências
  geom_point(alpha = 0.5) + # Pontos para cada resíduo
  labs(
    title = "Distribuição de Resíduos por Configuração ao Longo das Dimensões",
    x = "Dimensão",
    y = "Resíduo",
    color = "Configuração"
  ) +
  theme_minimal() +
  theme(legend.position = "top")
```

## Teste das Premissas

Os experimentos realizados e detalhados nas seções anteriores indicam que a configuração 1 (Config1) performa melhor do que a configuração 2 (Config2).
No entanto, esses experimentos só poderão ser validados se os resíduos dos mesmos seguirem as premissas do ANOVA: normalidade, independência e homoscedasticidade.
Nas seções seguintes, apresentaremos as estatíticas construídas para verificarmos cada uma das premissas citadas.

### Teste de Normalidade

Os resultados dos testes de normalidade indicam que, tanto para os resíduos combinados quanto para os resíduos separados por configuração, a hipótese nula do teste Shapiro-Wilk, que assume que os dados seguem uma distribuição normal, foi rejeitada (p-valor $\leq 0.05$).
O valor da estatística W, próximo de $0.9$ em todos os casos, reflete um desvio considerável em relação à normalidade.
Esses resultados sugerem que os resíduos, tanto no conjunto total quanto em cada configuração (Configuração 1 e Configuração 2), não apresentam distribuição normal.
O código utilizado para o cômputo dos testes de normalidade é apresentado abaixo:

```{r}
# Carregar todos os resíduos
residuals_matrix <- as.matrix(residuals_df)

# 1. Teste de normalidade para todos os resíduos combinados
all_residuals <- as.vector(residuals_matrix)
shapiro_all <- shapiro.test(all_residuals)
cat("Teste de Normalidade - Todos os Resíduos:\n")
print(shapiro_all)

# 2. Teste de normalidade para resíduos da Configuração 1
config1_columns <- grep("Residual_Config1", colnames(residuals_df), value = TRUE)
residuals_config1 <- as.vector(as.matrix(residuals_df[, config1_columns]))
shapiro_config1 <- shapiro.test(residuals_config1)
cat("\nTeste de Normalidade - Resíduos Configuração 1:\n")
print(shapiro_config1)

# 3. Teste de normalidade para resíduos da Configuração 2
config2_columns <- grep("Residual_Config2", colnames(residuals_df), value = TRUE)
residuals_config2 <- as.vector(as.matrix(residuals_df[, config2_columns]))
shapiro_config2 <- shapiro.test(residuals_config2)
cat("\nTeste de Normalidade - Resíduos Configuração 2:\n")
print(shapiro_config2)
```

### Teste de Independência

Para aplicar o teste de independência, o código mencionado calcula uma matriz de correlação, que verifica o grau de relação linear entre os resíduos de diferentes colunas.
A interpretação da matriz ajudará a entender se há independência (valores de correlação próximos de 0) ou dependência (valores próximos de -1 ou 1).

```{r}
# Carregar os resíduos normalizados (se necessário)
dados_normalizados <- residuals_matrix

# Calcular a matriz de correlação para os resíduos normalizados
matriz_correlacao <- cor(dados_normalizados, method = "pearson", use = "complete.obs")

# Análise resumida dos valores de correlação
cat("\nResumo da Correlação:\n")
summary(as.vector(matriz_correlacao))
```

Os resultados da matriz de correlação indicam que os resíduos apresentam alta correlação linear entre si, com valores variando de $0.9583$ a $1.0000$.
A mediana ($0.9836$) e a média ($0.9830$) reforçam a forte dependência linear entre as execuções analisadas.
Esses resultados sugerem que os resíduos não são independentes, indicando possível interação entre os fatores analisados ou padrões sistemáticos nos dados que influenciam as diferentes execuções.
Essa dependência deve ser considerada em análises subsequentes, especialmente se forem utilizados métodos que assumem independência dos resíduos, pois ela pode impactar a validade das conclusões estatísticas.
Ajustes no modelo ou métodos específicos para tratar a dependência podem ser necessários.

### Teste de Homogeneidade de Variância:

```{r}

dados_longo <- pivot_longer(
  residuals_df, 
  cols = starts_with("Residual_Config"), 
  names_to = c("Configuração", "Execução"),
  names_pattern = "Residual_(Config\\d+)_Run_(\\d+)",
  values_to = "Resíduo"
)


leveneTest(Resíduo ~ Configuração, data = dados_longo)
```

O Teste de Levene foi aplicado para verificar a homogeneidade de variâncias entre os grupos analisados, com o uso da mediana como medida de centralidade.
O teste resultou em um valor de p-valor significativamente menor do que o nível de significância adotado de $\alpha = 0.05$.
Dessa forma, rejeita-se a hipótese nula do teste ($H_0$), que afirma que as variâncias dos grupos são homogêneas.
Isso indica que há diferenças significativas nas variâncias entre os grupos analisados.

### Conclusão dos Teste das Premissas

Os testes estatísticos realizados indicaram que os resíduos não seguem uma distribuição normal (Shapiro-Wilk com p-valor $\leq 0.05$), apresentam alta dependência linear (média e mediana das correlações próximas a $0.98$), e não possuem homogeneidade de variâncias entre as configurações (Teste de Levene com p-valor $\leq 0.001$).
Esses resultados invalidam todas as premissas do ANOVA tradicional, não fornecendo fundamentação estatística para as conclusões extraídas acerca da performance das duas configurações da meta-heurística DE ao problema de otimização de funções Rosenbrock.

Seguiremos, portanto, com os chamados **Testes Não-Paramétricos**.
É importante ressaltar que tais testes são comumente utilizados em estudos estatísticos de algoritmos evolucionários [@Derrac2011], pois, como presenciamos até aqui no presente estudo de caso, tais algoritmos não costumam apresentar resultados com distribuições bem comportadas.


## Analise preliminar
```{r}
# Dados de exemplo: ajuste para usar seus próprios dados
config1_times <- as.vector(as.matrix(residuals_df[, grep("Residual_Config1", colnames(residuals_df))]))
config2_times <- as.vector(as.matrix(residuals_df[, grep("Residual_Config2", colnames(residuals_df))]))

# Criar um dataframe consolidado
data_visual <- data.frame(
  Tempo = c(config1_times, config2_times),
  Configuração = rep(c("Config1", "Config2"), c(length(config1_times), length(config2_times)))
)

# Gráfico de Boxplot
ggplot(data_visual, aes(x = Configuração, y = Tempo, fill = Configuração)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Comparação de Tempos entre Config1 e Config2",
       x = "Configuração",
       y = "Tempo",
       fill = "Configuração") +
  scale_fill_manual(values = c("Config1" = "skyblue", "Config2" = "tomato")) +
  theme(text = element_text(size = 12))

```

O gráfico de caixas apresenta uma comparação visual dos tempos entre as configurações Config1 e Config2.
Observa-se que a Config1 possui uma mediana significativamente menor, além de uma distribuição mais concentrada abaixo do eixo zero, indicando tempos menores e mais consistentes.
Em contraste, a Config2 apresenta valores de tempo consideravelmente mais elevados e maior dispersão, evidenciando um desempenho inferior.
A presença de outliers na Config1 sugere alguns valores atípicos, porém eles não comprometem a tendência geral.
Assim, a análise sugere que a Config1 supera a Config2, apresentando tempos menores e, consequentemente, melhor desempenho.


# Teste Não Paramétricos
Para determinar qual das duas configurações (Config1 e Config2) apresenta melhor desempenho em todos os cenários, optamos pelo Teste de Permutação.
Esse método foi escolhido devido às características dos dados, que não atendem às premissas de normalidade e homogeneidade de variâncias, conforme verificado nos testes de Shapiro-Wilk e Levene, respectivamente.
O Teste de Permutação é robusto, não exige suposições rígidas sobre a distribuição dos dados e permite a comparação direta das médias entre as configurações.
Ele utiliza reamostragens para construir uma distribuição empírica da estatística de teste, fornecendo uma análise confiável para identificar se as diferenças observadas entre as configurações são estatisticamente significativas.

## Análise da Diferença das Médias entre Configurações ao Longo das Dimensões

```{r}
# Calcular a diferença entre as médias das configurações diretamente no conjunto inicial
results_difs <- results_shuffled %>%
  mutate(Difference = Mean_Config1 - Mean_Config2)

ggplot(results_difs, aes(x = Dimension, y = Difference)) +
  geom_line(color = "blue", alpha = 0.7, size = 1) + # Linha interpolada entre os pontos
  geom_point(color = "red", alpha = 0.8, size = 2) + # Pontos para as dimensões específicas
  labs(
    title = "Diferença entre as Médias das Configurações ao Longo das Dimensões",
    x = "Dimensão",
    y = "Diferença (Configuração 1 - Configuração 2)"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5)
  )
```
O gráfico demonstra que a diferença entre os resultados das Configurações 1 e 2 ao longo do número de dimensões é sempre menor ou igual a zero. Isso indica que, em todos os cenários avaliados, a Configuração 2 apresentou médias iguais ou superiores às da Configuração 1, sem ocorrência de valores positivos na diferença.

## Teste de Wilcoxon

```{r}
shapiro_result <- shapiro.test(results_difs$Difference)
print(shapiro_result)

# Teste de hipótese para a média (t-test)
if (shapiro_result$p.value > 0.05) {
  # Se normalidade for confirmada
  t_test_result <- t.test(results_difs$Difference, mu = 0, alternative = "less")
} else {
  # Se não for normal
  t_test_result <- wilcox.test(results_difs$Difference, mu = 0, alternative = "less")
}
print(t_test_result)
```
O teste de normalidade de Shapiro-Wilk apresentou um valor de $ W = 0.78334$ e $ p \text{-value} = 1.24 \times 10^{-5} $, indicando que a hipótese nula de normalidade dos dados deve ser rejeitada. Isso significa que a distribuição das diferenças entre as médias das configurações não segue uma distribuição normal. Diante disso, foi utilizado o teste não paramétrico de Wilcoxon para comparar a mediana das diferenças com zero. O teste de Wilcoxon revelou um valor de $ V = 0 $ e $ p \text{-value} = 5.821 \times 10^{-11} $, rejeitando a hipótese nula de que a mediana das diferenças é igual a zero. Esses resultados confirmam, com alta confiabilidade, que as diferenças são consistentemente menores que zero, reforçando que a Configuração 2 supera a Configuração 1 ao longo de todas as dimensões analisadas.



# Conclusões

Nesse estudo de caso, utilizamos técnicas estatísticas para detectar diferenças na performance entre configurações da meta-heurística DE para a otimização de funções Rosenbrock.
Em particular, aplicamos a metodologia de blocagem (*Randomized Complete Block Design*) para eliminar efeitos indesejáveis que poderiam invalidar as conclusões extraídas a partir de testes e gráficos.

Os primeiros testes realizados assumiram que os dados seguiriam uma distribuição normal, o que foi refutado através da análise das premissas do teste ANOVA.
Dessa forma, estendemos o nosso estudo com os chamados testes não paramétricos para validar a nossa hipótese alternativa de que existe uma diferença na performance do DE quando executado com duas configurações diferentes.

Uma vez aceita a hipótese alternativa para diferença de performance entre os algoritmos, utilizamos a saída dos testes para determinar qual a melhor configuração do DE para a otimização de funções Rosenbrock.
Nessa análise, que consistiu na verificação dos tempos de execução de cada algoritmo, foi verificado que a configuração 1 (Config1) obteve performance melhor do que Config2.

Como trabalho futuro, sugerimos a utilização de outros testes estatísticos não paramétricos para a validação de tais resultados.
Além disso, é de interesse do grupo a realização de um experimento fatorial, para uma análise mais exaustiva das possibilidades de configuração da meta-heurística DE.

# Referências